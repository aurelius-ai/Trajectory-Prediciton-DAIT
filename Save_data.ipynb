{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10f4f74d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from LSTM import *\n",
    "from LSTMbis import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.utils.data as utils\n",
    "import time\n",
    "import os\n",
    "\n",
    "import pdb\n",
    "import pickle\n",
    "\n",
    "\n",
    "# For the notebook\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = {}\n",
    "data_val = {}\n",
    "a = 0\n",
    "b = 0\n",
    "c = 0\n",
    "max_len = 0\n",
    "for j in range(1,4):\n",
    "    for i in os.listdir('./final_dataset_no_aug/train/%s'%j):\n",
    "        if i.endswith('.txt'):\n",
    "            a = a+1\n",
    "            data_train['%s'%a] = pd.read_csv('./final_dataset_no_aug/train/{}/{}'.format(j,i), header = None, \n",
    "                                        names = ['frameNb','id', 'x','y','Vx','Vy'],\n",
    "                                           delimiter=' ')\n",
    "            if len(data_train['%s'%a])/20>max_len:\n",
    "                max_len=int(len(data_train['%s'%a])/20)\n",
    "        \n",
    "    for k in os.listdir('./final_dataset_no_aug/validation/%s'%j):\n",
    "        if k.endswith('.txt'):\n",
    "            b = b+1\n",
    "            data_val['%s'%b] = pd.read_csv('./final_dataset_no_aug/validation/{}/{}'.format(j,k), header = None, \n",
    "                                        names = ['frameNb','id', 'x','y','Vx','Vy'],\n",
    "                                           delimiter=' ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_1 = {}\n",
    "c = 0\n",
    "j = 1\n",
    "for i in os.listdir('./final_dataset_no_aug/test/%s'%j):\n",
    "    if i.endswith('.txt'):\n",
    "        c = c+1\n",
    "        data_test_1['%s'%c] = pd.read_csv('./final_dataset_no_aug/test/{}/{}'.format(j,i), header = None, \n",
    "                                    names = ['frameNb','id', 'x','y','Vx','Vy'],\n",
    "                                       delimiter=' ')\n",
    "\n",
    "data_test_2 = {}\n",
    "c = 0\n",
    "j = 2\n",
    "for i in os.listdir('./final_dataset_no_aug/test/%s'%j):\n",
    "    if i.endswith('.txt'):\n",
    "        c = c+1\n",
    "        data_test_2['%s'%c] = pd.read_csv('./final_dataset_no_aug/test/{}/{}'.format(j,i), header = None, \n",
    "                                    names = ['frameNb','id', 'x','y','Vx','Vy'],\n",
    "                                       delimiter=' ')\n",
    "\n",
    "data_test_3 = {}\n",
    "c = 0\n",
    "j = 3\n",
    "for i in os.listdir('./final_dataset_no_aug/test/%s'%j):\n",
    "    if i.endswith('.txt'):\n",
    "        c = c+1\n",
    "        data_test_3['%s'%c] = pd.read_csv('./final_dataset_no_aug/test/{}/{}'.format(j,i), header = None, \n",
    "                                    names = ['frameNb','id', 'x','y','Vx','Vy'],\n",
    "                                       delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1477, 10, 272])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 0\n",
    "inputs_train = np.zeros([10,len(data_train),4*max_len])\n",
    "gt_train = np.zeros([11,len(data_train),4])\n",
    "in_train_coord = np.zeros([10,len(data_train),2])\n",
    "gt_train_coord = np.zeros([11,len(data_train),2])\n",
    "\n",
    "for i in data_train:\n",
    "    len_d = np.int(len(data_train['%s'%i])/20)\n",
    "    inputs_train[:,a,:4] = data_train['%s'%i].loc[0:9,'x':'Vy']\n",
    "    for m in range(10):\n",
    "        frame = np.array(data_train['%s'%i].loc[m,'frameNb'])\n",
    "        no_int = 0\n",
    "        for j in range(1,len_d):\n",
    "            check = 0\n",
    "            for k in range(20):\n",
    "                if data_train['%s'%i].loc[j*20+k,'frameNb']==frame:\n",
    "                    inputs_train[m,(a),(j-no_int)*4:(j+1-no_int)*4]=data_train['%s'%i].loc[j*20+k,'x':'Vy']\n",
    "                    check+=1\n",
    "            if check == 0:\n",
    "                no_int+=1           \n",
    "\n",
    "        \n",
    "    gt_train[:,a,:] = np.array(data_train['%s'%i].loc[9:19,['x','y','Vx','Vy']])\n",
    "    in_train_coord[:,a,:] = np.array(data_train['%s'%i].loc[0:9,['x','y']])\n",
    "    gt_train_coord[:,a,:] = np.array(data_train['%s'%i].loc[9:19,['x','y']])\n",
    "    a +=1\n",
    "\n",
    "while sum(sum(sum(inputs_train[:,:,-4:]!=0)))==0:\n",
    "    inputs_train = inputs_train[:,:,:-4]\n",
    "\n",
    "inputs_train = torch.from_numpy(inputs_train).float()\n",
    "gt_train = torch.from_numpy(gt_train).float()\n",
    "inputs_train = inputs_train.permute([1,0,2])\n",
    "gt_train = gt_train.permute([1,0,2])\n",
    "inputs_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( inputs_train.numpy(), open( \"./import_dataset_no_aug/train/inputs_train.pkl\", \"wb\" ) )\n",
    "pickle.dump( gt_train.numpy(), open( \"./import_dataset_no_aug/train/gt_train.pkl\", \"wb\" ) )\n",
    "pickle.dump( in_train_coord, open( \"./import_dataset_no_aug/train/in_train_coord.pkl\", \"wb\" ) )\n",
    "pickle.dump( gt_train_coord, open( \"./import_dataset_no_aug/train/gt_train_coord.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VALIDATION SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "inputs_validation = np.zeros([10,len(data_val),inputs_train[:,:,:].shape[2]])\n",
    "gt_validation = np.zeros([11,len(data_val),4])\n",
    "in_validation_coord = np.zeros([10,len(data_val),2])\n",
    "gt_validation_coord = np.zeros([11,len(data_val),2])\n",
    "\n",
    "for i in data_val:\n",
    "    len_d = np.int(len(data_val['%s'%i])/20)\n",
    "    inputs_validation[:,a,:4] = data_val['%s'%i].loc[0:9,'x':'Vy']\n",
    "    for m in range(10):\n",
    "        frame = np.array(data_val['%s'%i].loc[m,'frameNb'])  \n",
    "        no_int = 0\n",
    "        for j in range(1,len_d):\n",
    "            check = 0            \n",
    "            for k in range(20):\n",
    "                if data_val['%s'%i].loc[j*20+k,'frameNb']==frame:\n",
    "                    inputs_validation[m,a,(j-no_int)*4:(j+1-no_int)*4]=data_val['%s'%i].loc[j*20+k,'x':'Vy']\n",
    "                    check+=1\n",
    "            if check == 0:\n",
    "                no_int+=1  \n",
    "    gt_validation[:,a,:] = np.array(data_val['%s'%i].loc[9:19,['x','y','Vx','Vy']])\n",
    "    in_validation_coord[:,a,:] = np.array(data_val['%s'%i].loc[0:9,['x','y']])\n",
    "    gt_validation_coord[:,a,:] = np.array(data_val['%s'%i].loc[9:19,['x','y']])\n",
    "    a +=1\n",
    "\n",
    "\n",
    "inputs_validation = torch.from_numpy(inputs_validation).float()\n",
    "gt_validation = torch.from_numpy(gt_validation).float()\n",
    "inputs_validation = inputs_validation.permute([1,0,2])\n",
    "gt_validation = gt_validation.permute([1,0,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( inputs_validation.numpy(), open( \"./import_dataset_no_aug/validation/inputs_validation.pkl\", \"wb\" ) )\n",
    "pickle.dump( gt_validation.numpy(), open( \"./import_dataset_no_aug/validation/gt_validation.pkl\", \"wb\" ) )\n",
    "pickle.dump( in_validation_coord, open( \"./import_dataset_no_aug/validation/in_validation_coord.pkl\", \"wb\" ) )\n",
    "pickle.dump( gt_validation_coord, open( \"./import_dataset_no_aug/validation/gt_validation_coord.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "inputs_test_1 = np.zeros([10,len(data_test_1),4*max_len])\n",
    "gt_test_1 = np.zeros([11,len(data_test_1),4])\n",
    "in_test_coord_1 = np.zeros([10,len(data_test_1),2])\n",
    "gt_test_coord_1 = np.zeros([11,len(data_test_1),2])\n",
    "\n",
    "for i in data_test_1:\n",
    "    len_d = np.int(len(data_test_1['%s'%i])/20)\n",
    "    inputs_test_1[:,a,:4] = data_test_1['%s'%i].loc[0:9,'x':'Vy']\n",
    "    for m in range(10):\n",
    "        frame = np.array(data_test_1['%s'%i].loc[m,'frameNb'])\n",
    "        no_int=0\n",
    "        for j in range(1,len_d):\n",
    "            check = 0\n",
    "            for k in range(20):\n",
    "                if data_test_1['%s'%i].loc[j*20+k,'frameNb']==frame:\n",
    "                    inputs_test_1[m,a,j*4:(j+1)*4]=data_test_1['%s'%i].loc[j*20+k,'x':'Vy']\n",
    "                    check+=1\n",
    "            if check == 0:\n",
    "                no_int+=1 \n",
    "                \n",
    "    gt_test_1[:,a,:] = np.array(data_test_1['%s'%i].loc[9:19,['x','y','Vx','Vy']])\n",
    "    in_test_coord_1[:,a,:] = np.array(data_test_1['%s'%i].loc[0:9,['x','y']])\n",
    "    gt_test_coord_1[:,a,:] = np.array(data_test_1['%s'%i].loc[9:19,['x','y']])\n",
    "    a +=1\n",
    "\n",
    "\n",
    "inputs_test_1 = torch.from_numpy(inputs_test_1).float()\n",
    "gt_test_1 = torch.from_numpy(gt_test_1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( inputs_test_1.numpy(), open( \"./import_dataset_no_aug/test/inputs_test_1.pkl\", \"wb\" ) )\n",
    "pickle.dump( gt_test_1.numpy(), open( \"./import_dataset_no_aug/test/gt_test_1.pkl\", \"wb\" ) )\n",
    "pickle.dump( in_test_coord_1, open( \"./import_dataset_no_aug/test/in_test_coord_1.pkl\", \"wb\" ) )\n",
    "pickle.dump( gt_test_coord_1, open( \"./import_dataset_no_aug/test/gt_test_coord_1.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-19-8e694775879e>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-8e694775879e>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    check+=1\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "inputs_test_2 = np.zeros([10,len(data_test_2),4*max_len])\n",
    "gt_test_2 = np.zeros([11,len(data_test_2),4])\n",
    "in_test_coord_2 = np.zeros([10,len(data_test_2),2])\n",
    "gt_test_coord_2 = np.zeros([11,len(data_test_2),2])\n",
    "\n",
    "for i in data_test_2:\n",
    "    len_d = np.int(len(data_test_2['%s'%i])/20)\n",
    "    inputs_test_2[:,a,:4] = data_test_2['%s'%i].loc[0:9,'x':'Vy']\n",
    "    for m in range(10):\n",
    "        frame = np.array(data_test_2['%s'%i].loc[m,'frameNb'])  \n",
    "        no_int=0\n",
    "        for j in range(1,len_d):\n",
    "            check = 0\n",
    "            for k in range(20):\n",
    "                if data_test_2['%s'%i].loc[j*20+k,'frameNb']==frame:\n",
    "                    inputs_test_2[m,a,(j-no_int)*4:(j+1-no_int)*4]=data_test_2['%s'%i].loc[j*20+k,'x':'Vy']\n",
    "                    check+=1\n",
    "            if check ==0:\n",
    "                no_int+=1\n",
    "                    \n",
    "    gt_test_2[:,a,:] = np.array(data_test_2['%s'%i].loc[9:19,['x','y','Vx','Vy']])\n",
    "    in_test_coord_2[:,a,:] = np.array(data_test_2['%s'%i].loc[0:9,['x','y']])\n",
    "    gt_test_coord_2[:,a,:] = np.array(data_test_2['%s'%i].loc[9:19,['x','y']])\n",
    "    a +=1\n",
    "\n",
    "\n",
    "inputs_test_2 = torch.from_numpy(inputs_test_2).float()\n",
    "gt_test_2 = torch.from_numpy(gt_test_2).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( inputs_test_2.numpy(), open( \"./import_dataset_no_aug/test/inputs_test_2.pkl\", \"wb\" ) )\n",
    "pickle.dump( gt_test_2.numpy(), open( \"./import_dataset_no_aug/test/gt_test_2.pkl\", \"wb\" ) )\n",
    "pickle.dump( in_test_coord_2, open( \"./import_dataset_no_aug/test/in_test_coord_2.pkl\", \"wb\" ) )\n",
    "pickle.dump( gt_test_coord_2, open( \"./import_dataset_no_aug/test/gt_test_coord_2.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "inputs_test_3 = np.zeros([10,len(data_test_3),4*max_len])\n",
    "gt_test_3 = np.zeros([11,len(data_test_3),4])\n",
    "in_test_coord_3 = np.zeros([10,len(data_test_3),2])\n",
    "gt_test_coord_3 = np.zeros([11,len(data_test_3),2])\n",
    "\n",
    "no_int = 0\n",
    "for i in data_test_3:\n",
    "    len_d = np.int(len(data_test_3['%s'%i])/20)\n",
    "    inputs_test_3[:,a,:4] = data_test_3['%s'%i].loc[0:9,'x':'Vy']\n",
    "    for m in range(10):\n",
    "        frame = np.array(data_test_3['%s'%i].loc[m,'frameNb'])  \n",
    "        for j in range(1,len_d):\n",
    "            check = 0\n",
    "            for k in range(20):\n",
    "                if data_test_3['%s'%i].loc[j*20+k,'frameNb']==frame:\n",
    "                    inputs_test_3[m,a,(j-no_int)*4:(j+1-no_int)*4]=data_test_3['%s'%i].loc[j*20+k,'x':'Vy']\n",
    "                    check+=1\n",
    "            if check == 0:\n",
    "                no_int+=1\n",
    "                \n",
    "    \n",
    "    gt_test_3[:,a,:] = np.array(data_test_3['%s'%i].loc[9:19,['x','y','Vx','Vy']])\n",
    "    in_test_coord_3[:,a,:] = np.array(data_test_3['%s'%i].loc[0:9,['x','y']])\n",
    "    gt_test_coord_3[:,a,:] = np.array(data_test_3['%s'%i].loc[9:19,['x','y']])\n",
    "    a +=1\n",
    "\n",
    "\n",
    "inputs_test_3 = torch.from_numpy(inputs_test_3).float()\n",
    "gt_test_3 = torch.from_numpy(gt_test_3).float()\n",
    "no_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( inputs_test_3.numpy(), open( \"./import_dataset_no_aug/test/inputs_test_3.pkl\", \"wb\" ) )\n",
    "pickle.dump( gt_test_3.numpy(), open( \"./import_dataset_no_aug/test/gt_test_3.pkl\", \"wb\" ) )\n",
    "pickle.dump( in_test_coord_3, open( \"./import_dataset_no_aug/test/in_test_coord_3.pkl\", \"wb\" ) )\n",
    "pickle.dump( gt_test_coord_3, open( \"./import_dataset_no_aug/test/gt_test_coord_3.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF SAVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "lr = 0.01\n",
    "lstm = LSTM(4*max_len,4,num_layers=2,hidden_size=128)\n",
    "optimizer = optim.SGD(lstm.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "traindataset = utils.TensorDataset(inputs_train, gt_train[:,1:,:])\n",
    "trainloader = utils.DataLoader(traindataset, batch_size=16, shuffle=True)\n",
    "\n",
    "valdataset = utils.TensorDataset(inputs_validation, gt_validation[:,1:,:])\n",
    "valloader = utils.DataLoader(valdataset, batch_size=16, shuffle=True)\n",
    "\n",
    "epochs = 50\n",
    "steps = 0\n",
    "print_every = 323\n",
    "running_loss = 0 #### DOD\n",
    "\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "for e in range(epochs):\n",
    "    start = time.time()\n",
    "    total_train_loss=0\n",
    "    steps_bis = 0\n",
    "    if (e+1)%10==0:\n",
    "        lr /= 5\n",
    "        optimizer = optim.SGD(lstm.parameters(), lr=lr)\n",
    "\n",
    "    for train_coord, ground_tru in iter(trainloader):\n",
    "        steps += 1\n",
    "        steps_bis+=1\n",
    "        \n",
    "        train_coord = train_coord.permute([1,0,2])\n",
    "        ground_tru = ground_tru.permute([1,0,2])\n",
    "\n",
    "        in_train = Variable(train_coord)\n",
    "        targets = Variable(ground_tru)\n",
    "        optimizer.zero_grad()\n",
    "        #print(in_train.shape)\n",
    "        #print(targets.shape)\n",
    "        out = lstm.forward(in_train)\n",
    "        out_bis = out[:,:,0:2].clone()\n",
    "        #print(out_bis.shape)    \n",
    "        for i in range(10):\n",
    "            if i == 0:\n",
    "                out_bis[i, :, 0:2] = in_train[-1, :, 0:2] + out[i, :, 2:]*0.4\n",
    "            else:\n",
    "                out_bis[i, :, 0:2] = out[i - 1, :, 0:2] + out[i, :, 2:]*0.4\n",
    "        #pdb.set_trace()\n",
    "        loss1 = (criterion(out[:,:,0:2], targets[:,:,0:2]))\n",
    "        loss2 = (criterion(out[:,:,2:], targets[:,:,2:]))\n",
    "        loss3 = criterion(out_bis, targets[:,:,0:2])\n",
    "        loss4 = 5*criterion(out[0,:,2:], targets[0,:,2:])\n",
    "                                #+ 10*criterion(out_bis[-1,:,:], targets[-1,:,0:2]) \n",
    "                                #+ 10*criterion(out_bis[0,:,:], targets[0,:,0:2]))\n",
    "        loss1.backward(retain_graph=True)\n",
    "        loss2.backward(retain_graph=True)\n",
    "        loss3.backward(retain_graph=True)\n",
    "        loss4.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += (loss1+loss2+loss3+loss4).data[0]\n",
    "        total_train_loss += (loss1+loss2+loss3+loss4).data[0]\n",
    "    \n",
    "        if steps % print_every == 0:\n",
    "                stop = time.time()\n",
    "                val_loss=0\n",
    "                for ii, (valcoord, valgt) in enumerate(valloader):\n",
    "                    inputs = Variable(valcoord, volatile=True)\n",
    "                    predicted = lstm.predict(inputs)\n",
    "                    val_loss+= criterion(predicted,valgt).data[0]\n",
    "                    \n",
    "                print(\"Epoch: {}/{}..\".format(e+1, epochs),\n",
    "                  \"Validation loss: {:.4f}..\".format(val_loss/ii),\n",
    "                  \"Training loss: {:.4f}..\".format(running_loss/print_every),\n",
    "                  \"{:.4f} s/batch\".format((stop - start)/print_every)\n",
    "                 )\n",
    "                loss_val.append(val_loss/ii)\n",
    "                running_loss = 0\n",
    "                start = time.time()\n",
    "    loss_train.append(total_train_loss/steps_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test = lstm.predict(inputs_test)\n",
    "output_test = output_test.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post processing step\n",
    "Go back to coordinate:\n",
    "We have Vx and Vy and we want x and y.\n",
    "$ V = d/t$\n",
    "$ d = V*t$\n",
    "Here t = 0.4s between each point.\n",
    "Start from data at index 9. Then we add d_x and d_y to the last x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_coord=np.zeros([11,len(data_test),2])\n",
    "for j in range(11):\n",
    "    for i in range(len(data_test)):\n",
    "        if j==0:\n",
    "            output_coord[j,i,0:2] = in_test_coord[9,i,0:2]\n",
    "        else:\n",
    "            output_coord[j,i,0:2] = output_coord[j-1,i,0:2]+output_test[j-1,i,2:]*0.4\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.zeros(len(data_test))\n",
    "for i in range(len(data_test)):\n",
    "    dist[i] = np.sqrt(sum((gt_test_coord[10,i,:]-output_coord[10,i,:])**2))\n",
    "\n",
    "final_coord_error = np.mean(dist)\n",
    "print('The final distance between the ground trought and the predicted coordinates is :',final_coord_error.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avr = np.zeros((len(data_test),11))\n",
    "for i in range(len(data_test)):\n",
    "    for j in range(11):\n",
    "        avr[i,j] = np.sqrt(sum((gt_test_coord[j,i,:]-output_coord[j,i,:])**2))\n",
    "\n",
    "average = np.mean(np.mean(avr,1))\n",
    "print('The average error between the ground trought and the predicted coordinates is :',average.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    plt.figure(figsize=(12, 7))\n",
    "\n",
    "    plt.plot(in_test_coord[:,(i*20)+800,0],in_test_coord[:,i*20+800,1],c='b')\n",
    "    plt.plot(gt_test_coord[:,i*20+800,0],gt_test_coord[:,i*20+800,1],c='k')\n",
    "    plt.plot(output_coord[:,i*20+800,0],output_coord[:,i*20+800,1],c='r')\n",
    "    plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = np.arange(1,epochs+1)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('font', size=20)\n",
    "\n",
    "plt.plot(epoch,loss_train,label='Training loss')\n",
    "plt.plot(epoch,loss_val,c='k',label='Validation loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = np.arange(1,epochs+1,0.5)\n",
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
