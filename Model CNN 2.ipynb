{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "\n",
    "import helper\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "matplotlib.rcParams['text.latex.unicode'] = True\n",
    "\n",
    "# For the notebook\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biwi = pd.read_csv('./data/train/biwi/biwi_hotel.txt', header = None,\n",
    "                 names = ['frameNb','id', 'x','y'],delimiter=' ')\n",
    "id_unique = np.unique(np.array(biwi['id']))\n",
    "\n",
    "init = np.zeros(len(biwi)) \n",
    "biwi['Speed'] = init\n",
    "biwi['Angle'] = init\n",
    "biwi['Vx'] = init\n",
    "biwi['Vy'] = init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None ## Disable StettingWithCopy warning\n",
    "c = 0\n",
    "for i in id_unique:\n",
    "    a = biwi[biwi['id']==i]\n",
    "    ind = a.index\n",
    "    a.index = range(len(a))\n",
    "    dist = a['x'].iloc\n",
    "    dist1 = a.loc[0:len(a)-2,'x':'y']\n",
    "    dist1.index=range(len(dist1))\n",
    "    dist2 = a.loc[1:,'x':'y']\n",
    "    dist2.index=range(len(dist2))\n",
    "    dist = dist2-dist1\n",
    "    b = len(dist)\n",
    "    if c < b:\n",
    "        vector_speed = np.zeros((len(id_unique),b,2))\n",
    "        c=b\n",
    "b = 0\n",
    "for i in id_unique:\n",
    "    a = biwi[biwi['id']==i]\n",
    "    ind = a.index\n",
    "    a.index = range(len(a))\n",
    "    dist = a['x'].iloc\n",
    "    dist1 = a.loc[0:len(a)-2,'x':'y']\n",
    "    dist1.index=range(len(dist1))\n",
    "    dist2 = a.loc[1:,'x':'y']\n",
    "    dist2.index=range(len(dist2))\n",
    "    dist = dist2-dist1\n",
    "    speed = np.array(np.sqrt(dist['x']**2+dist['y']**2)/0.4)\n",
    "    biwi.loc[ind[1:],'Speed'] = speed\n",
    "    angle=np.zeros(len(dist)-1)\n",
    "    vx=np.zeros(len(dist))\n",
    "    vy=np.zeros(len(dist))\n",
    "    for j in range(len(dist)-1):\n",
    "        if norm(dist.loc[j,:])==0 or norm(dist.loc[j+1,:])==0:\n",
    "            angle[j]=0\n",
    "        elif np.cross(dist.loc[j,:],dist.loc[j+1,:])/(norm(dist.loc[j,:])*norm(dist.loc[j+1,:]))>1:\n",
    "            angle[j]=np.arcsin(1)\n",
    "        else:\n",
    "            angle[j]=np.arcsin(np.cross(dist.loc[j,:],dist.loc[j+1,:])/(norm(dist.loc[j,:])*norm(dist.loc[j+1,:])))\n",
    "    \n",
    "    for j in range(len(dist)):\n",
    "        if j == 0:\n",
    "            vx[j] = 0\n",
    "            vy[j] = speed[j]\n",
    "            vector_speed[b][j][0]=vx[j]\n",
    "            vector_speed[b][j][1]=vy[j]\n",
    "        else:\n",
    "            vx[j] = speed[j]*np.sin(sum(angle[:j]))\n",
    "            vy[j] = speed[j]*np.cos(sum(angle[:j]))\n",
    "            vector_speed[b][j][0]=vx[j]\n",
    "            vector_speed[b][j][1]=vy[j]\n",
    "    \n",
    "    biwi.loc[ind[2:],'Angle'] = angle\n",
    "    biwi.loc[ind[1:],'Vx'] = vx\n",
    "    biwi.loc[ind[1:],'Vy'] = vy\n",
    "    b+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.zeros((len(vector_speed),2*len(vector_speed[0])))\n",
    "x_train = np.zeros((len(vector_speed),20))\n",
    "y_train = np.zeros((len(vector_speed),18))\n",
    "for i in range(len(vector_speed)):\n",
    "    vector[i][:] = np.reshape(vector_speed[i][:][:],2*19,'C')\n",
    "for i in range(len(vector)):\n",
    "    x_train[i][:] = vector[i][:20]\n",
    "    y_train[i][:] = vector[i][20:38]\n",
    "\n",
    "x_train,y_train = torch.from_numpy(x_train).type(torch.FloatTensor), torch.from_numpy(y_train).type(torch.FloatTensor)\n",
    "\n",
    "traindataset = utils.TensorDataset(x_train, y_train)\n",
    "trainloader = utils.DataLoader(traindataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, n_input_channels=1, n_output=None):\n",
    "        super().__init__()\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Define 2 or more different layers of the neural network                      #\n",
    "        ################################################################################\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(n_input_channels,8,5,padding=2)\n",
    "        #self.conv1_bn = nn.BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True)\n",
    "     \n",
    "        self.fc1 = nn.Linear(19*2 * 8, 19*2 * 8)\n",
    "        \n",
    "        \n",
    "        ################################################################################\n",
    "        #                              END OF YOUR CODE                                #\n",
    "        ################################################################################\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Set up the forward pass that the input data will go through.                 #\n",
    "        # A good activation function betweent the layers is a ReLu function.           #\n",
    "        #                                                                              #\n",
    "        # Note that the output of the last convolution layer should be flattened       #\n",
    "        # before being inputted to the fully connected layer. We can flatten           #\n",
    "        # Variable `x` with `x.view`.                                                  #\n",
    "        ################################################################################\n",
    "        \n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = x.view(-1, 19*2 * 40) # in order to reshape the tensor for as many columns we need\n",
    "        x = self.fc1(x)\n",
    "        ################################################################################\n",
    "        #                              END OF YOUR CODE                                #\n",
    "        ################################################################################\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "    def predict(self, x):\n",
    "        logits = self.forward(x)\n",
    "        return F.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 3D tensor as input, got 2D tensor instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-70769d94916f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#                              END OF YOUR CODE                                #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-79edc5b8a23a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Max pooling over a (2, 2) window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# in order to reshape the tensor for as many columns we need\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 168\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv1d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \"\"\"\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected 3D tensor as input, got {}D tensor instead.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     f = _ConvNd(_single(stride), _single(padding), _single(dilation), False,\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 3D tensor as input, got 2D tensor instead."
     ]
    }
   ],
   "source": [
    "net = ConvNet()\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Choose an Optimizer that will be used to minimize the loss function.         #\n",
    "# Choose a critera that measures the loss                                      #\n",
    "################################################################################\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "epochs = 1\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 50\n",
    "for e in range(epochs):\n",
    "    start = time.time()\n",
    "    for images, labels in iter(trainloader):\n",
    "        steps += 1\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Run the training process                                                     #\n",
    "        #                                                                              #\n",
    "        # HINT: Do not forget to transform the inputs and outputs into Variable        #\n",
    "        # which pytorch uses.                                                          #\n",
    "        ################################################################################\n",
    "        inputs = Variable(images)\n",
    "        targets = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        output = net.forward(inputs)\n",
    "        ################################################################################\n",
    "        #                              END OF YOUR CODE                                #\n",
    "        ################################################################################\n",
    "        \n",
    "        loss = criterion(output, targets)\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Run the training process                                                     #\n",
    "        #                                                                              #\n",
    "        # HINT: Calculate the gradient and move one step further                       #\n",
    "        ################################################################################\n",
    "        grad = loss.backward()\n",
    "        optimizer.step()\n",
    "        ################################################################################\n",
    "        #                              END OF YOUR CODE                                #\n",
    "        ################################################################################\n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            stop = time.time()\n",
    "            # Test accuracy\n",
    "            accuracy = 0\n",
    "            for ii, (images, labels) in enumerate(valloader):\n",
    "                ################################################################################\n",
    "                # TODO:                                                                        #\n",
    "                # Calculate the accuracy                                                       #\n",
    "                ################################################################################\n",
    "                \n",
    "                inputs = Variable(images, volatile=True)\n",
    "                \n",
    "                predicted = net.predict(inputs).data\n",
    "                equality = (labels == predicted.max(1)[1])\n",
    "                accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "                \n",
    "                \n",
    "                #im = Variable(images)\n",
    "                #out = net.predict(im)\n",
    "                #_,prediction = torch.max(out, 1)\n",
    "                #pred_y = prediction.data.numpy().squeeze()\n",
    "                #target_y = labels.numpy()\n",
    "                #accuracy = np.mean(pred_y == target_y)\n",
    "                #print(pred_y.shape,target_y.shape)\n",
    "                \n",
    "                \n",
    "                ################################################################################\n",
    "                #                              END OF YOUR CODE                                #\n",
    "                ################################################################################\n",
    "            \n",
    "            print(\"Epoch: {}/{}..\".format(e+1, epochs),\n",
    "                  \"Loss: {:.4f}..\".format(running_loss/print_every),\n",
    "                  \"Test accuracy: {:.4f}..\".format(accuracy/(ii+1)),\n",
    "                  \"{:.4f} s/batch\".format((stop - start)/print_every)\n",
    "                 )\n",
    "            running_loss = 0\n",
    "            start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "          ...             ⋱             ...          \n",
       " 0.0000  1.4372 -0.0261  ...   1.4368  0.1313  1.5607\n",
       " 0.0000  1.5768 -0.0273  ...   1.4698 -0.3532  1.5185\n",
       " 0.0000  1.3730 -0.1302  ...   1.5778 -0.3114  1.5441\n",
       "[torch.FloatTensor of size 145x20]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frameNb</th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Angle</th>\n",
       "      <th>Vx</th>\n",
       "      <th>Vy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>70</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>130</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>140</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>160</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>170</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>180</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>190</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>90</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2870</th>\n",
       "      <td>17850</td>\n",
       "      <td>413</td>\n",
       "      <td>1.85</td>\n",
       "      <td>-3.43</td>\n",
       "      <td>1.559046</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>-0.353171</td>\n",
       "      <td>1.518517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2871</th>\n",
       "      <td>17860</td>\n",
       "      <td>413</td>\n",
       "      <td>2.08</td>\n",
       "      <td>-2.82</td>\n",
       "      <td>1.629801</td>\n",
       "      <td>-0.084469</td>\n",
       "      <td>-0.501812</td>\n",
       "      <td>1.550624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>17870</td>\n",
       "      <td>413</td>\n",
       "      <td>2.26</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>1.590008</td>\n",
       "      <td>0.073627</td>\n",
       "      <td>-0.376954</td>\n",
       "      <td>1.544678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2873</th>\n",
       "      <td>17880</td>\n",
       "      <td>413</td>\n",
       "      <td>2.39</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>1.583706</td>\n",
       "      <td>0.080256</td>\n",
       "      <td>-0.250906</td>\n",
       "      <td>1.563704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>17890</td>\n",
       "      <td>413</td>\n",
       "      <td>2.52</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>1.657181</td>\n",
       "      <td>0.009288</td>\n",
       "      <td>-0.247339</td>\n",
       "      <td>1.638619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>17900</td>\n",
       "      <td>413</td>\n",
       "      <td>2.61</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>1.566246</td>\n",
       "      <td>0.053241</td>\n",
       "      <td>-0.151019</td>\n",
       "      <td>1.558948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2876</th>\n",
       "      <td>17910</td>\n",
       "      <td>413</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.554228</td>\n",
       "      <td>-0.050087</td>\n",
       "      <td>-0.227124</td>\n",
       "      <td>1.537543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2877</th>\n",
       "      <td>17920</td>\n",
       "      <td>413</td>\n",
       "      <td>2.87</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.564649</td>\n",
       "      <td>-0.031360</td>\n",
       "      <td>-0.277067</td>\n",
       "      <td>1.539922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878</th>\n",
       "      <td>17930</td>\n",
       "      <td>413</td>\n",
       "      <td>3.01</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.491643</td>\n",
       "      <td>-0.011248</td>\n",
       "      <td>-0.280634</td>\n",
       "      <td>1.465007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>17940</td>\n",
       "      <td>413</td>\n",
       "      <td>3.22</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.518634</td>\n",
       "      <td>-0.116142</td>\n",
       "      <td>-0.456625</td>\n",
       "      <td>1.448359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>17770</td>\n",
       "      <td>414</td>\n",
       "      <td>2.79</td>\n",
       "      <td>-10.10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2881</th>\n",
       "      <td>17780</td>\n",
       "      <td>414</td>\n",
       "      <td>2.69</td>\n",
       "      <td>-9.56</td>\n",
       "      <td>1.372953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.372953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2882</th>\n",
       "      <td>17790</td>\n",
       "      <td>414</td>\n",
       "      <td>2.63</td>\n",
       "      <td>-8.95</td>\n",
       "      <td>1.532359</td>\n",
       "      <td>-0.085066</td>\n",
       "      <td>-0.130194</td>\n",
       "      <td>1.526818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2883</th>\n",
       "      <td>17800</td>\n",
       "      <td>414</td>\n",
       "      <td>2.66</td>\n",
       "      <td>-8.25</td>\n",
       "      <td>1.751606</td>\n",
       "      <td>-0.140876</td>\n",
       "      <td>-0.392402</td>\n",
       "      <td>1.707087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884</th>\n",
       "      <td>17810</td>\n",
       "      <td>414</td>\n",
       "      <td>2.63</td>\n",
       "      <td>-7.57</td>\n",
       "      <td>1.701654</td>\n",
       "      <td>0.086920</td>\n",
       "      <td>-0.235806</td>\n",
       "      <td>1.685236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885</th>\n",
       "      <td>17820</td>\n",
       "      <td>414</td>\n",
       "      <td>2.68</td>\n",
       "      <td>-6.89</td>\n",
       "      <td>1.704589</td>\n",
       "      <td>-0.117486</td>\n",
       "      <td>-0.432462</td>\n",
       "      <td>1.648818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886</th>\n",
       "      <td>17830</td>\n",
       "      <td>414</td>\n",
       "      <td>2.71</td>\n",
       "      <td>-6.26</td>\n",
       "      <td>1.576785</td>\n",
       "      <td>0.025814</td>\n",
       "      <td>-0.360537</td>\n",
       "      <td>1.535012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>17840</td>\n",
       "      <td>414</td>\n",
       "      <td>2.73</td>\n",
       "      <td>-5.57</td>\n",
       "      <td>1.725724</td>\n",
       "      <td>0.018606</td>\n",
       "      <td>-0.363268</td>\n",
       "      <td>1.687057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>17850</td>\n",
       "      <td>414</td>\n",
       "      <td>2.82</td>\n",
       "      <td>-4.89</td>\n",
       "      <td>1.714825</td>\n",
       "      <td>-0.102611</td>\n",
       "      <td>-0.530790</td>\n",
       "      <td>1.630609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2889</th>\n",
       "      <td>17860</td>\n",
       "      <td>414</td>\n",
       "      <td>2.81</td>\n",
       "      <td>-4.25</td>\n",
       "      <td>1.600195</td>\n",
       "      <td>0.147212</td>\n",
       "      <td>-0.266761</td>\n",
       "      <td>1.577803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>17870</td>\n",
       "      <td>414</td>\n",
       "      <td>2.82</td>\n",
       "      <td>-3.62</td>\n",
       "      <td>1.575198</td>\n",
       "      <td>-0.031495</td>\n",
       "      <td>-0.311373</td>\n",
       "      <td>1.544117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2891</th>\n",
       "      <td>17880</td>\n",
       "      <td>414</td>\n",
       "      <td>2.89</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>1.708984</td>\n",
       "      <td>-0.086708</td>\n",
       "      <td>-0.481626</td>\n",
       "      <td>1.639714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2892</th>\n",
       "      <td>17890</td>\n",
       "      <td>414</td>\n",
       "      <td>2.84</td>\n",
       "      <td>-2.46</td>\n",
       "      <td>1.206493</td>\n",
       "      <td>0.206372</td>\n",
       "      <td>-0.095597</td>\n",
       "      <td>1.202700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>17900</td>\n",
       "      <td>414</td>\n",
       "      <td>2.80</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>1.154340</td>\n",
       "      <td>-0.017054</td>\n",
       "      <td>-0.111074</td>\n",
       "      <td>1.148983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2894</th>\n",
       "      <td>17910</td>\n",
       "      <td>414</td>\n",
       "      <td>2.81</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>1.200260</td>\n",
       "      <td>-0.107569</td>\n",
       "      <td>-0.243089</td>\n",
       "      <td>1.175386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>17920</td>\n",
       "      <td>414</td>\n",
       "      <td>2.75</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.308625</td>\n",
       "      <td>0.135707</td>\n",
       "      <td>-0.089224</td>\n",
       "      <td>1.305580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2896</th>\n",
       "      <td>17930</td>\n",
       "      <td>414</td>\n",
       "      <td>2.75</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>-0.114877</td>\n",
       "      <td>-0.282238</td>\n",
       "      <td>1.524087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>17940</td>\n",
       "      <td>414</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.526843</td>\n",
       "      <td>-0.049141</td>\n",
       "      <td>-0.351432</td>\n",
       "      <td>1.485848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2898</th>\n",
       "      <td>17950</td>\n",
       "      <td>414</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.550806</td>\n",
       "      <td>0.081388</td>\n",
       "      <td>-0.233074</td>\n",
       "      <td>1.533192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>17960</td>\n",
       "      <td>414</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.507481</td>\n",
       "      <td>-0.131916</td>\n",
       "      <td>-0.420626</td>\n",
       "      <td>1.447610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2900 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      frameNb   id     x      y     Speed     Angle        Vx        Vy\n",
       "0           0    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "1          10    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "2          20    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "3          30    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "4          40    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "5          50    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "6          60    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "7          70    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "8          80    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "9          90    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "10        100    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "11        110    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "12        120    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "13        130    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "14        140    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "15        150    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "16        160    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "17        170    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "18        180    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "19        190    5 -1.59   0.93  0.000000  0.000000  0.000000  0.000000\n",
       "20          0    6 -1.72   1.32  0.000000  0.000000  0.000000  0.000000\n",
       "21         10    6 -1.72   1.32  0.000000  0.000000  0.000000  0.000000\n",
       "22         20    6 -1.72   1.32  0.000000  0.000000  0.000000  0.000000\n",
       "23         30    6 -1.72   1.32  0.000000  0.000000  0.000000  0.000000\n",
       "24         40    6 -1.72   1.32  0.000000  0.000000  0.000000  0.000000\n",
       "25         50    6 -1.72   1.32  0.000000  0.000000  0.000000  0.000000\n",
       "26         60    6 -1.72   1.32  0.000000  0.000000  0.000000  0.000000\n",
       "27         70    6 -1.72   1.32  0.000000  0.000000  0.000000  0.000000\n",
       "28         80    6 -1.72   1.32  0.000000  0.000000  0.000000  0.000000\n",
       "29         90    6 -1.72   1.32  0.000000  0.000000  0.000000  0.000000\n",
       "...       ...  ...   ...    ...       ...       ...       ...       ...\n",
       "2870    17850  413  1.85  -3.43  1.559046  0.024823 -0.353171  1.518517\n",
       "2871    17860  413  2.08  -2.82  1.629801 -0.084469 -0.501812  1.550624\n",
       "2872    17870  413  2.26  -2.21  1.590008  0.073627 -0.376954  1.544678\n",
       "2873    17880  413  2.39  -1.59  1.583706  0.080256 -0.250906  1.563704\n",
       "2874    17890  413  2.52  -0.94  1.657181  0.009288 -0.247339  1.638619\n",
       "2875    17900  413  2.61  -0.32  1.566246  0.053241 -0.151019  1.558948\n",
       "2876    17910  413  2.73   0.29  1.554228 -0.050087 -0.227124  1.537543\n",
       "2877    17920  413  2.87   0.90  1.564649 -0.031360 -0.277067  1.539922\n",
       "2878    17930  413  3.01   1.48  1.491643 -0.011248 -0.280634  1.465007\n",
       "2879    17940  413  3.22   2.05  1.518634 -0.116142 -0.456625  1.448359\n",
       "2880    17770  414  2.79 -10.10  0.000000  0.000000  0.000000  0.000000\n",
       "2881    17780  414  2.69  -9.56  1.372953  0.000000  0.000000  1.372953\n",
       "2882    17790  414  2.63  -8.95  1.532359 -0.085066 -0.130194  1.526818\n",
       "2883    17800  414  2.66  -8.25  1.751606 -0.140876 -0.392402  1.707087\n",
       "2884    17810  414  2.63  -7.57  1.701654  0.086920 -0.235806  1.685236\n",
       "2885    17820  414  2.68  -6.89  1.704589 -0.117486 -0.432462  1.648818\n",
       "2886    17830  414  2.71  -6.26  1.576785  0.025814 -0.360537  1.535012\n",
       "2887    17840  414  2.73  -5.57  1.725724  0.018606 -0.363268  1.687057\n",
       "2888    17850  414  2.82  -4.89  1.714825 -0.102611 -0.530790  1.630609\n",
       "2889    17860  414  2.81  -4.25  1.600195  0.147212 -0.266761  1.577803\n",
       "2890    17870  414  2.82  -3.62  1.575198 -0.031495 -0.311373  1.544117\n",
       "2891    17880  414  2.89  -2.94  1.708984 -0.086708 -0.481626  1.639714\n",
       "2892    17890  414  2.84  -2.46  1.206493  0.206372 -0.095597  1.202700\n",
       "2893    17900  414  2.80  -2.00  1.154340 -0.017054 -0.111074  1.148983\n",
       "2894    17910  414  2.81  -1.52  1.200260 -0.107569 -0.243089  1.175386\n",
       "2895    17920  414  2.75  -1.00  1.308625  0.135707 -0.089224  1.305580\n",
       "2896    17930  414  2.75  -0.38  1.550000 -0.114877 -0.282238  1.524087\n",
       "2897    17940  414  2.78   0.23  1.526843 -0.049141 -0.351432  1.485848\n",
       "2898    17950  414  2.76   0.85  1.550806  0.081388 -0.233074  1.533192\n",
       "2899    17960  414  2.82   1.45  1.507481 -0.131916 -0.420626  1.447610\n",
       "\n",
       "[2900 rows x 8 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
