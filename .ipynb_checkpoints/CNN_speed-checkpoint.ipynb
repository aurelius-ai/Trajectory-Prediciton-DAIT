{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10f145790>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from CNN2D import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.utils.data as utils\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import pdb\n",
    "\n",
    "# For the notebook\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5157, 1, 10, 162]), torch.Size([5157, 1, 11, 2]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_train_coord = pickle.load( open( \"./import_dataset_2/train/gt_train_coord.pkl\", \"rb\" ) )\n",
    "gt_train  = pickle.load( open( \"./import_dataset_2/train/gt_train.pkl\", \"rb\" ) )\n",
    "in_train_coord  = pickle.load( open( \"./import_dataset_2/train/in_train_coord.pkl\", \"rb\" ) )\n",
    "inputs_train = pickle.load( open( \"./import_dataset_2/train/inputs_train.pkl\", \"rb\" ) )\n",
    "\n",
    "inputs_train = inputs_train[:,:,2:]\n",
    "gt_train = gt_train[:,:,2:]\n",
    "\n",
    "\n",
    "inputs_train = torch.from_numpy(inputs_train).float()\n",
    "gt_train = torch.from_numpy(gt_train).float()\n",
    "\n",
    "gt_train = gt_train.unsqueeze(1) # add 1 dimension to the training set\n",
    "inputs_train = inputs_train.unsqueeze(1) # add 1 dimension to the training set\n",
    "\n",
    "inputs_train.shape,gt_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VALIDATION SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1719, 1, 10, 162])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((11, 1719, 2),\n",
       " torch.Size([1719, 1, 11, 2]),\n",
       " (10, 1719, 2),\n",
       " torch.Size([1719, 1, 10, 162]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_validation_coord = pickle.load( open( \"./import_dataset_2/validation/gt_validation_coord.pkl\", \"rb\" ) )\n",
    "gt_validation = pickle.load( open( \"./import_dataset_2/validation/gt_validation.pkl\", \"rb\" ) )\n",
    "in_validation_coord = pickle.load( open( \"./import_dataset_2/validation/in_validation_coord.pkl\", \"rb\" ) )\n",
    "inputs_validation = pickle.load( open( \"./import_dataset_2/validation/inputs_validation.pkl\", \"rb\" ) )\n",
    "\n",
    "inputs_validation = inputs_validation[:,:,2:]\n",
    "gt_validation = gt_validation[:,:,2:]\n",
    "\n",
    "inputs_validation = torch.from_numpy(inputs_validation).float()\n",
    "gt_validation = torch.from_numpy(gt_validation).float()\n",
    "\n",
    "gt_validation = gt_validation.unsqueeze(1) # add 1 dimension to the training set\n",
    "inputs_validation = inputs_validation.unsqueeze(1) # add 1 dimension to the training set\n",
    "print (inputs_validation.shape)\n",
    "\n",
    "gt_validation_coord.shape,gt_validation.shape,in_validation_coord.shape,inputs_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 460, 164])\n"
     ]
    }
   ],
   "source": [
    "gt_test_coord_1 = pickle.load( open( \"./import_dataset_2/test/gt_test_coord_1.pkl\", \"rb\" ) )\n",
    "gt_test_1 = pickle.load( open( \"./import_dataset_2/test/gt_test_1.pkl\", \"rb\" ) )\n",
    "in_test_coord_1 = pickle.load( open( \"./import_dataset_2/test/in_test_coord_1.pkl\", \"rb\" ) )\n",
    "inputs_test_1 = pickle.load( open( \"./import_dataset_2/test/inputs_test_1.pkl\", \"rb\" ) )\n",
    "\n",
    "gt_test_coord_2 = pickle.load( open( \"./import_dataset_2/test/gt_test_coord_2.pkl\", \"rb\" ) )\n",
    "gt_test_2 = pickle.load( open( \"./import_dataset_2/test/gt_test_2.pkl\", \"rb\" ) )\n",
    "in_test_coord_2 = pickle.load( open( \"./import_dataset_2/test/in_test_coord_2.pkl\", \"rb\" ) )\n",
    "inputs_test_2 = pickle.load( open( \"./import_dataset_2/test/inputs_test_2.pkl\", \"rb\" ) )\n",
    "\n",
    "gt_test_coord_3 = pickle.load( open( \"./import_dataset_2/test/gt_test_coord_3.pkl\", \"rb\" ) )\n",
    "gt_test_3 = pickle.load( open( \"./import_dataset_2/test/gt_test_3.pkl\", \"rb\" ) )\n",
    "in_test_coord_3 = pickle.load( open( \"./import_dataset_2/test/in_test_coord_3.pkl\", \"rb\" ) )\n",
    "inputs_test_3 = pickle.load( open( \"./import_dataset_2/test/inputs_test_3.pkl\", \"rb\" ) )\n",
    "\n",
    "inputs_test_1 = torch.from_numpy(inputs_test_1).float()\n",
    "gt_test_1 = torch.from_numpy(gt_test_1).float()\n",
    "inputs_test_1 = inputs_test_1.unsqueeze(1) \n",
    "\n",
    "inputs_test_2 = torch.from_numpy(inputs_test_2).float()\n",
    "gt_test_2 = torch.from_numpy(gt_test_2).float()\n",
    "inputs_test_2 = inputs_test_2.unsqueeze(1)\n",
    "\n",
    "inputs_test_3 = torch.from_numpy(inputs_test_3).float()\n",
    "gt_test_3 = torch.from_numpy(gt_test_3).float()\n",
    "inputs_test_3 = inputs_test_3.unsqueeze(1)\n",
    "print (inputs_test_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CNN2D()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "traindataset = utils.TensorDataset(inputs_train, gt_train[:,:,1:,:])\n",
    "trainloader = utils.DataLoader(traindataset, batch_size=16, shuffle=True)\n",
    "\n",
    "valdataset = utils.TensorDataset(inputs_validation, gt_validation[:,:,1:,:])\n",
    "valloader = utils.DataLoader(valdataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50.. Validation Loss: 2.7657.. Training Loss: 5.1102.. 0.0075 s/batch \n",
      "Epoch: 2/50.. Validation Loss: 2.0632.. Training Loss: 2.3528.. 0.0096 s/batch \n",
      "Epoch: 3/50.. Validation Loss: 1.6608.. Training Loss: 1.8363.. 0.0091 s/batch \n",
      "Epoch: 4/50.. Validation Loss: 1.3565.. Training Loss: 1.4791.. 0.0093 s/batch \n",
      "Epoch: 5/50.. Validation Loss: 1.1662.. Training Loss: 1.2290.. 0.0248 s/batch \n",
      "Epoch: 6/50.. Validation Loss: 1.0177.. Training Loss: 1.0373.. 0.0331 s/batch \n",
      "Epoch: 7/50.. Validation Loss: 0.9060.. Training Loss: 0.9110.. 0.0319 s/batch \n",
      "Epoch: 8/50.. Validation Loss: 0.8192.. Training Loss: 0.7957.. 0.0329 s/batch \n",
      "Epoch: 9/50.. Validation Loss: 0.7984.. Training Loss: 0.7417.. 0.0329 s/batch \n",
      "Epoch: 10/50.. Validation Loss: 0.7129.. Training Loss: 0.6845.. 0.0322 s/batch \n",
      "Epoch: 11/50.. Validation Loss: 0.7406.. Training Loss: 0.6411.. 0.0329 s/batch \n",
      "Epoch: 12/50.. Validation Loss: 0.6877.. Training Loss: 0.6265.. 0.0333 s/batch \n",
      "Epoch: 13/50.. Validation Loss: 0.6434.. Training Loss: 0.5919.. 0.0327 s/batch \n",
      "Epoch: 14/50.. Validation Loss: 0.6537.. Training Loss: 0.5710.. 0.0334 s/batch \n",
      "Epoch: 15/50.. Validation Loss: 0.6171.. Training Loss: 0.5628.. 0.0335 s/batch \n",
      "Epoch: 16/50.. Validation Loss: 0.6029.. Training Loss: 0.5402.. 0.0333 s/batch \n",
      "Epoch: 17/50.. Validation Loss: 0.6065.. Training Loss: 0.5200.. 0.0332 s/batch \n",
      "Epoch: 18/50.. Validation Loss: 0.5609.. Training Loss: 0.5127.. 0.0341 s/batch \n",
      "Epoch: 19/50.. Validation Loss: 0.5919.. Training Loss: 0.5017.. 0.0340 s/batch \n",
      "Epoch: 20/50.. Validation Loss: 0.5632.. Training Loss: 0.4890.. 0.0337 s/batch \n",
      "Epoch: 21/50.. Validation Loss: 0.5697.. Training Loss: 0.4962.. 0.0345 s/batch \n",
      "Epoch: 22/50.. Validation Loss: 0.5360.. Training Loss: 0.4696.. 0.0349 s/batch \n",
      "Epoch: 23/50.. Validation Loss: 0.5747.. Training Loss: 0.4650.. 0.0350 s/batch \n",
      "Epoch: 24/50.. Validation Loss: 0.5223.. Training Loss: 0.4552.. 0.0359 s/batch \n",
      "Epoch: 25/50.. Validation Loss: 0.5254.. Training Loss: 0.4575.. 0.0375 s/batch \n",
      "Epoch: 26/50.. Validation Loss: 0.5090.. Training Loss: 0.4488.. 0.0397 s/batch \n",
      "Epoch: 27/50.. Validation Loss: 0.5274.. Training Loss: 0.4392.. 0.0361 s/batch \n",
      "Epoch: 28/50.. Validation Loss: 0.5171.. Training Loss: 0.4391.. 0.0369 s/batch \n",
      "Epoch: 29/50.. Validation Loss: 0.5082.. Training Loss: 0.4390.. 0.0372 s/batch \n",
      "Epoch: 30/50.. Validation Loss: 0.5204.. Training Loss: 0.4250.. 0.0385 s/batch \n",
      "Epoch: 31/50.. Validation Loss: 0.5194.. Training Loss: 0.4159.. 0.0371 s/batch \n",
      "Epoch: 32/50.. Validation Loss: 0.5007.. Training Loss: 0.4176.. 0.0394 s/batch \n",
      "Epoch: 33/50.. Validation Loss: 0.5096.. Training Loss: 0.4218.. 0.0380 s/batch \n",
      "Epoch: 34/50.. Validation Loss: 0.4683.. Training Loss: 0.3987.. 0.0376 s/batch \n",
      "Epoch: 35/50.. Validation Loss: 0.4726.. Training Loss: 0.3921.. 0.0371 s/batch \n",
      "Epoch: 36/50.. Validation Loss: 0.4814.. Training Loss: 0.3967.. 0.0379 s/batch \n",
      "Epoch: 37/50.. Validation Loss: 0.4705.. Training Loss: 0.3818.. 0.0379 s/batch \n",
      "Epoch: 38/50.. Validation Loss: 0.6208.. Training Loss: 0.3862.. 0.0380 s/batch \n",
      "Epoch: 39/50.. Validation Loss: 0.4426.. Training Loss: 0.3803.. 0.0376 s/batch \n",
      "Epoch: 40/50.. Validation Loss: 0.4752.. Training Loss: 0.3756.. 0.0386 s/batch \n",
      "Epoch: 41/50.. Validation Loss: 0.4575.. Training Loss: 0.3821.. 0.0394 s/batch \n",
      "Epoch: 42/50.. Validation Loss: 0.4658.. Training Loss: 0.3690.. 0.0399 s/batch \n",
      "Epoch: 43/50.. Validation Loss: 0.4698.. Training Loss: 0.3740.. 0.0375 s/batch \n",
      "Epoch: 44/50.. Validation Loss: 0.4677.. Training Loss: 0.3573.. 0.0376 s/batch \n",
      "Epoch: 45/50.. Validation Loss: 0.4512.. Training Loss: 0.3561.. 0.0377 s/batch \n",
      "Epoch: 46/50.. Validation Loss: 0.4434.. Training Loss: 0.3537.. 0.0374 s/batch \n",
      "Epoch: 47/50.. Validation Loss: 0.4478.. Training Loss: 0.3515.. 0.0370 s/batch \n",
      "Epoch: 48/50.. Validation Loss: 0.4400.. Training Loss: 0.3492.. 0.0379 s/batch \n",
      "Epoch: 49/50.. Validation Loss: 0.4653.. Training Loss: 0.3441.. 0.0376 s/batch \n",
      "Epoch: 50/50.. Validation Loss: 0.4770.. Training Loss: 0.3518.. 0.0376 s/batch \n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 323\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    start = time.time()\n",
    "    steps_bis = 0\n",
    "    total_train_loss=0\n",
    "    for train_coord, ground_tru in iter(trainloader):\n",
    "        \n",
    "        steps += 1\n",
    "        steps_bis += 1\n",
    "        \n",
    "        input_train = Variable(train_coord)\n",
    "        in_train = input_train.squeeze(1)\n",
    "        target = Variable(ground_tru)\n",
    "        targets = target.squeeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        out = net.forward(input_train)\n",
    "        out = out.reshape(out.shape[0],10,4)\n",
    "        loss1 = (criterion(out[:,:,0:2], targets[:,:,0:2]))\n",
    "        \n",
    "        loss1.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += (loss1).item()\n",
    "        total_train_loss += (loss1).item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            stop = time.time()\n",
    "            val_loss=0\n",
    "            \n",
    "            for ii, (images, labels) in enumerate(valloader):\n",
    "                \n",
    "                inp = Variable(images)\n",
    "                input_val = inp.squeeze(1)\n",
    "                lab = Variable(labels)\n",
    "                valgt = lab.squeeze(1)\n",
    "                predicted = net.predict(inp)\n",
    "                predicted = predicted.reshape(predicted.shape[0],10,4)\n",
    "                val_loss+= (criterion(predicted[:,:,0:2],valgt[:,:,0:2]).item())\n",
    "                \n",
    "            print(\"Epoch: {}/{}..\".format(e+1, epochs),\n",
    "                  \"Validation Loss: {:.4f}..\".format(val_loss/ii),\n",
    "                  \"Training Loss: {:.4f}..\".format(running_loss/print_every),\n",
    "                  \"{:.4f} s/batch \".format((stop - start)/print_every),\n",
    "                 )\n",
    "            loss_val.append(val_loss/ii)\n",
    "            running_loss = 0\n",
    "            start = time.time()\n",
    "    loss_train.append(total_train_loss/steps_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50.. Validation Loss: 5.4016.. Training Loss: 9.8787.. 0.0133 s/batch \n",
      "Epoch: 2/50.. Validation Loss: 4.0012.. Training Loss: 4.5562.. 0.0127 s/batch \n",
      "Epoch: 3/50.. Validation Loss: 3.2259.. Training Loss: 3.5469.. 0.0132 s/batch \n",
      "Epoch: 4/50.. Validation Loss: 2.6600.. Training Loss: 2.8733.. 0.0121 s/batch \n",
      "Epoch: 5/50.. Validation Loss: 2.3207.. Training Loss: 2.4181.. 0.0118 s/batch \n",
      "Epoch: 6/50.. Validation Loss: 2.0507.. Training Loss: 2.0674.. 0.0122 s/batch \n",
      "Epoch: 7/50.. Validation Loss: 1.8434.. Training Loss: 1.8329.. 0.0124 s/batch \n",
      "Epoch: 8/50.. Validation Loss: 1.6846.. Training Loss: 1.6129.. 0.0118 s/batch \n",
      "Epoch: 9/50.. Validation Loss: 1.6496.. Training Loss: 1.5113.. 0.0118 s/batch \n",
      "Epoch: 10/50.. Validation Loss: 1.4773.. Training Loss: 1.4036.. 0.0118 s/batch \n",
      "Epoch: 11/50.. Validation Loss: 1.5370.. Training Loss: 1.3196.. 0.0118 s/batch \n",
      "Epoch: 12/50.. Validation Loss: 1.4445.. Training Loss: 1.2924.. 0.0119 s/batch \n",
      "Epoch: 13/50.. Validation Loss: 1.3441.. Training Loss: 1.2234.. 0.0124 s/batch \n",
      "Epoch: 14/50.. Validation Loss: 1.3609.. Training Loss: 1.1808.. 0.0124 s/batch \n",
      "Epoch: 15/50.. Validation Loss: 1.2902.. Training Loss: 1.1636.. 0.0118 s/batch \n",
      "Epoch: 16/50.. Validation Loss: 1.2597.. Training Loss: 1.1174.. 0.0118 s/batch \n",
      "Epoch: 17/50.. Validation Loss: 1.2626.. Training Loss: 1.0746.. 0.0118 s/batch \n",
      "Epoch: 18/50.. Validation Loss: 1.1764.. Training Loss: 1.0607.. 0.0118 s/batch \n",
      "Epoch: 19/50.. Validation Loss: 1.2257.. Training Loss: 1.0364.. 0.0120 s/batch \n",
      "Epoch: 20/50.. Validation Loss: 1.1989.. Training Loss: 1.0099.. 0.0123 s/batch \n",
      "Epoch: 21/50.. Validation Loss: 1.1965.. Training Loss: 1.0248.. 0.0122 s/batch \n",
      "Epoch: 22/50.. Validation Loss: 1.1229.. Training Loss: 0.9711.. 0.0118 s/batch \n",
      "Epoch: 23/50.. Validation Loss: 1.2160.. Training Loss: 0.9598.. 0.0118 s/batch \n",
      "Epoch: 24/50.. Validation Loss: 1.1073.. Training Loss: 0.9396.. 0.0134 s/batch \n",
      "Epoch: 25/50.. Validation Loss: 1.1010.. Training Loss: 0.9441.. 0.0132 s/batch \n",
      "Epoch: 26/50.. Validation Loss: 1.0704.. Training Loss: 0.9270.. 0.0121 s/batch \n",
      "Epoch: 27/50.. Validation Loss: 1.1101.. Training Loss: 0.9065.. 0.0118 s/batch \n",
      "Epoch: 28/50.. Validation Loss: 1.0920.. Training Loss: 0.9070.. 0.0119 s/batch \n",
      "Epoch: 29/50.. Validation Loss: 1.0698.. Training Loss: 0.9068.. 0.0122 s/batch \n",
      "Epoch: 30/50.. Validation Loss: 1.0928.. Training Loss: 0.8779.. 0.0123 s/batch \n",
      "Epoch: 31/50.. Validation Loss: 1.1073.. Training Loss: 0.8599.. 0.0118 s/batch \n",
      "Epoch: 32/50.. Validation Loss: 1.0510.. Training Loss: 0.8641.. 0.0118 s/batch \n",
      "Epoch: 33/50.. Validation Loss: 1.0602.. Training Loss: 0.8693.. 0.0118 s/batch \n",
      "Epoch: 34/50.. Validation Loss: 0.9968.. Training Loss: 0.8243.. 0.0118 s/batch \n",
      "Epoch: 35/50.. Validation Loss: 1.0061.. Training Loss: 0.8130.. 0.0118 s/batch \n",
      "Epoch: 36/50.. Validation Loss: 1.0151.. Training Loss: 0.8194.. 0.0123 s/batch \n",
      "Epoch: 37/50.. Validation Loss: 0.9931.. Training Loss: 0.7897.. 0.0122 s/batch \n",
      "Epoch: 38/50.. Validation Loss: 1.3179.. Training Loss: 0.7995.. 0.0118 s/batch \n",
      "Epoch: 39/50.. Validation Loss: 0.9381.. Training Loss: 0.7896.. 0.0118 s/batch \n",
      "Epoch: 40/50.. Validation Loss: 1.0090.. Training Loss: 0.7771.. 0.0118 s/batch \n",
      "Epoch: 41/50.. Validation Loss: 0.9715.. Training Loss: 0.7922.. 0.0118 s/batch \n",
      "Epoch: 42/50.. Validation Loss: 0.9939.. Training Loss: 0.7652.. 0.0120 s/batch \n",
      "Epoch: 43/50.. Validation Loss: 0.9991.. Training Loss: 0.7756.. 0.0124 s/batch \n",
      "Epoch: 44/50.. Validation Loss: 0.9915.. Training Loss: 0.7425.. 0.0122 s/batch \n",
      "Epoch: 45/50.. Validation Loss: 0.9728.. Training Loss: 0.7389.. 0.0118 s/batch \n",
      "Epoch: 46/50.. Validation Loss: 0.9489.. Training Loss: 0.7359.. 0.0118 s/batch \n",
      "Epoch: 47/50.. Validation Loss: 0.9504.. Training Loss: 0.7294.. 0.0118 s/batch \n",
      "Epoch: 48/50.. Validation Loss: 0.9535.. Training Loss: 0.7243.. 0.0119 s/batch \n",
      "Epoch: 49/50.. Validation Loss: 0.9856.. Training Loss: 0.7171.. 0.0126 s/batch \n",
      "Epoch: 50/50.. Validation Loss: 0.9916.. Training Loss: 0.7300.. 0.0124 s/batch \n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 323\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    start = time.time()\n",
    "    steps_bis = 0\n",
    "    total_train_loss=0\n",
    "    for train_coord, ground_tru in iter(trainloader):\n",
    "        \n",
    "        steps += 1\n",
    "        steps_bis += 1\n",
    "        \n",
    "        input_train = Variable(train_coord)\n",
    "        in_train = input_train.squeeze(1)\n",
    "        target = Variable(ground_tru)\n",
    "        targets = target.squeeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        #print(in_train.shape)\n",
    "        #print(targets.shape)\n",
    "        out = net.forward(input_train)\n",
    "        out = out.reshape(out.shape[0],10,4)\n",
    "        out_bis = out[:,:,0:2].clone()\n",
    "        #print(out_bis.shape)   \n",
    "        for i in range(10):\n",
    "            if i == 0:\n",
    "                out_bis[:, i, 0:2] = in_train[:, -1, 0:2] + out[:, i, 2:]*0.4\n",
    "            else:\n",
    "                out_bis[:, i, 0:2] = out[:, i - 1, 0:2] + out[:, i, 2:]*0.4\n",
    "        #pdb.set_trace()\n",
    "        loss1 = (criterion(out[:,:,0:2], targets[:,:,0:2]))\n",
    "        loss2 = (criterion(out[:,:,2:], targets[:,:,2:]))\n",
    "        loss3 = criterion(out_bis, targets[:,:,0:2])\n",
    "        \n",
    "        loss1.backward(retain_graph=True)\n",
    "        loss2.backward(retain_graph=True)\n",
    "        loss3.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += (loss1+loss2+loss3).item()\n",
    "        total_train_loss += (loss1+loss2+loss3).item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            stop = time.time()\n",
    "            val_loss=0\n",
    "            \n",
    "            for ii, (images, labels) in enumerate(valloader):\n",
    "                \n",
    "                inp = Variable(images)\n",
    "                input_val = inp.squeeze(1)\n",
    "                lab = Variable(labels)\n",
    "                valgt = lab.squeeze(1)\n",
    "                predicted = net.predict(inp)\n",
    "                predicted = predicted.reshape(predicted.shape[0],10,4)\n",
    "                predicted_bis = predicted[:,:,0:2].clone()\n",
    "                #print(out_bis.shape)   \n",
    "                for i in range(10):\n",
    "                    if i == 0:\n",
    "                        predicted_bis[:, i, 0:2] = input_val[:, -1, 0:2] + predicted[:, i, 2:]*0.4\n",
    "                    else:\n",
    "                        predicted_bis[:, i, 0:2] = predicted[:, i - 1, 0:2] + predicted[:, i, 2:]*0.4\n",
    "                val_loss+= (criterion(predicted[:,:,0:2],valgt[:,:,0:2]).item()\n",
    "                                + criterion(predicted[:,:,2:],valgt[:,:,2:]).item()\n",
    "                                + criterion(predicted_bis, valgt[:,:,0:2]).item())\n",
    "                \n",
    "            print(\"Epoch: {}/{}..\".format(e+1, epochs),\n",
    "                  \"Validation Loss: {:.4f}..\".format(val_loss/ii),\n",
    "                  \"Training Loss: {:.4f}..\".format(running_loss/print_every),\n",
    "                  \"{:.4f} s/batch \".format((stop - start)/print_every),\n",
    "                 )\n",
    "            loss_val.append(val_loss/ii)\n",
    "            running_loss = 0\n",
    "            start = time.time()\n",
    "    loss_train.append(total_train_loss/steps_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
